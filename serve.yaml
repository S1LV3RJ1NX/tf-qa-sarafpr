name: llm-qa-backend
type: service
env:
  OPENAI_API_KEY: <OPENAI_API_KEY>
  ML_REPO_NAME: <ML_REPO_NAME>
  VECTOR_DB_CONFIG: <VECTOR_DB_CONFIG>
  METADATA_STORE_TYPE: <METADATA_STORE_TYPE>
  TFY_SERVICE_ROOT_PATH: <TFY_SERVICE_ROOT_PATH>
  JOB_FQN: <JOB_FQN>
  TFY_API_KEY: <TFY_API_KEY>
  TFY_HOST: <TFY_HOST>
  LLM_GATEWAY_ENDPOINT: <LLM_GATEWAY_ENDPOINT>
ports:
  - port: 8000
    expose: false
    protocol: TCP
    app_protocol: http
image:
  type: build
  build_spec:
    type: dockerfile
    command: uvicorn --host 0.0.0.0 --port 8000 backend.server.app:app
    dockerfile_path: ./backend/Dockerfile
    build_context_path: ./
  build_source:
    type: local
    local_build: false
replicas: 1
resources:
  cpu_request: 0.5
  cpu_limit: 0.5
  memory_request: 1500
  memory_limit: 2000
  ephemeral_storage_request: 2000
  ephemeral_storage_limit: 2000

