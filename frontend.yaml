name: llm-qa-frontend
type: service
ports:
  - host: <host for frontend>
    port: 8000
    path: /app/
    expose: true
    protocol: TCP
    app_protocol: http
env:
  ML_REPO_NAME: <ML_REPO_NAME>
  TFY_API_KEY: <TFY_API_KEY>
  TFY_HOST: <TFY_HOST>
  BACKEND_URL: http://llm-qa-backend.<workspace_name>.svc.cluster.local:8000
  LLM_GATEWAY_ENDPOINT: <LLM_GATEWAY_ENDPOINT>
  TRUEFOUNDRY_EMBEDDINGS_ENDPOINT: <TRUEFOUNDRY_EMBEDDINGS_ENDPOINT>
image:
  type: build
  build_spec:
    type: dockerfile
    dockerfile_path: ./frontend/Dockerfile
    build_context_path: ./frontend/
  build_source:
    type: local
    local_build: false
replicas: 1
resources:
  cpu_request: 0.5
  cpu_limit: 0.5
  memory_request: 1000
  memory_limit: 1000
  ephemeral_storage_request: 2000
  ephemeral_storage_limit: 2000
